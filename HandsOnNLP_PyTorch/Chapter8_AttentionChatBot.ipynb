{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c76883",
   "metadata": {},
   "source": [
    "# Chapter 8: Building a Chatbot Using Attention-Based Neural Networks (Notes)\n",
    "\n",
    "- chatbot logic similar to seq2seq modeling but now rather, we r training it to respond based on the input sequence.\n",
    "- adding attention\n",
    "- ability to learn where in the input to took to obtain information needed than using whole seq\n",
    "\n",
    "## Theory of Attention with Neural Networks\n",
    "\n",
    "- in prev seq2seq modeling, we have an input then an encoder which obtains the hidden state and then a decoder that interprets the hidden state to produce an output\n",
    "- however, decoding HS is not efficient as HS contains all of the sentence information (excessive)\n",
    "- just parts of the sentence is relevant is prediction\n",
    "- attention makes the model look at only relevant parts to make prediction, more efficient and accurate\n",
    "\n",
    "## Comparing local and global attention\n",
    "- 2 main types of attention mechanism: local and global\n",
    "\n",
    "### local:\n",
    "- model only knows a few HS from encoder\n",
    "- basically local weights calculated from a few states based on whats needed\n",
    "- need aligned position Pt from final HS which tells which HS to be looking at for pred\n",
    "- we then calculate local weight and apply that to our HS to find context vector\n",
    "- this weight will tell us to play attention to the more relevant features and less so the others\n",
    "- this is passed into the decoder to make preds (before we send in final HS which has alot ofinformation)\n",
    "\n",
    "### global:\n",
    "- similar but now global weights calculated from all states unlike local\n",
    "- allows our model to look at any given part of the input sentence that it considers relevant\n",
    "- global attention framework is that it is essentially learning a mask that only allows through hidden states that are relevant to our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6819441",
   "metadata": {},
   "source": [
    "# Building a ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fd0ab",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- dialog data is good for chatbots, as it trains the model to respond like how humans would\n",
    "- here we are using movie script data instead \n",
    "- transform a script of n lines into n-1 pairs of input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1e1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import absolute_import\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd140ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4432c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(\"Data\", \"formatted_movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbdd73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(datafile, 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "for line in lines[:3]:\n",
    "    print(str(line) + '\\n')\n",
    "    \n",
    "'''\n",
    "call and response halves of each line are separated by a tab delimiter (/t) \n",
    "and that each of our lines is separated by a new line delimiter (/n).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3bdf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0 \n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1        \n",
    "        \n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        words_to_keep = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "\n",
    "        print('Words to Keep: {} / {} = {:.2%}'.format(\n",
    "            len(words_to_keep), len(self.word2index), len(words_to_keep) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8783ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def cleanString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def readVocs(datafile, corpus_name):\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    pairs = [[cleanString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Vocabulary(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "def filterPair(p, max_length):\n",
    "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
    "\n",
    "def filterPairs(pairs, max_length):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]\n",
    "\n",
    "def loadData(corpus, corpus_name, datafile, max_length):\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(str(len(pairs)) + \" Sentence pairs\")\n",
    "    pairs = filterPairs(pairs,max_length)\n",
    "    print(str(len(pairs))+ \" Sentence pairs after trimming\")\n",
    "    for p in pairs:\n",
    "        voc.addSentence(p[0])\n",
    "        voc.addSentence(p[1])\n",
    "    print(str(voc.num_words) + \" Distinct words in vocabulary\")\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9666d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282 Sentence pairs\n",
      "111344 Sentence pairs after trimming\n",
      "26856 Distinct words in vocabulary\n",
      "CPU times: user 7.76 s, sys: 22.4 ms, total: 7.78 s\n",
      "Wall time: 7.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "max_length = 15\n",
    "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b51043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Pairs:\n",
      "['do you have a reservation ?', 'food ! !']\n",
      "['food ! !', 'i m sorry sir . we only seat by reservation .']\n",
      "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
      "['an historical moment gentlemen .', 'excuse me my lord . norris newman of the standard my lord .']\n",
      "['excuse me my lord . norris newman of the standard my lord .', 'saw you lead our cavalry sir']\n",
      "['saw you lead our cavalry sir', 'indeedldid mylord . itwas one ofthe first to cross .']\n",
      "['indeedldid mylord . itwas one ofthe first to cross .', 'were they in good heart as they entered enemy territory ?']\n",
      "['what o clock is it mr noggs ?', 'eleven o clock my lorj']\n",
      "['splendid horsemanship who are they ?', 'sikali horse my lord . christians all i know each one by name .']\n",
      "['sikali horse my lord . christians all i know each one by name .', 'they come well recommended do they ? durnford']\n",
      "['are you dictating the strategy of this war sir ?', 'i m explaining my reasons .']\n",
      "['splendid site crealock splendil i want to establish camp here immediately .', 'certainly sin']\n",
      "['stuart ?', 'yes .']\n",
      "['yes .', 'how quickly can you move your artillery forward ?']\n",
      "['well fed or hungry pulleine wants them in position immediately . .', 'right . bombardier to me please .']\n",
      "['do you think she might be interested in someone ?', 'which one ?']\n",
      "['which one ?', 'well that one . the one who keeps looking at me .']\n",
      "['well that one . the one who keeps looking at me .', 'ft could be you flatter yourself coghill it s that odd eye .']\n",
      "['your orders mr vereker ?', 'i m to take the sikali with the main column to the river']\n",
      "['i m to take the sikali with the main column to the river', 'lord chelmsford seems to want me to stay back with my basutos .']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Pairs:\")\n",
    "for pair in pairs[-20:]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2320951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRareWords(voc, all_pairs, minimum):\n",
    "    voc.trim(minimum)\n",
    "    \n",
    "    pairs_to_keep = []\n",
    "    \n",
    "    for p in all_pairs:\n",
    "        keep = True\n",
    "        \n",
    "        for word in p[0].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        for word in p[1].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "\n",
    "        if keep:\n",
    "            pairs_to_keep.append(p)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.2%} of total\".format(len(all_pairs)\\\n",
    "        , len(pairs_to_keep), len(pairs_to_keep)/ len(all_pairs)))\n",
    "    return pairs_to_keep\n",
    "\n",
    "\n",
    "minimum_count = 3\n",
    "pairs = removeRareWords(voc, pairs, minimum_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexFromSentence(voc, sent):\n",
    "    return [voc.word2index[w] for w in sent.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPad(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    return padTensor, lengths\n",
    "\n",
    "def getMask(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    mask = torch.BoolTensor(getMask(padList))\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    return padTensor, mask, max_target_len\n",
    "\n",
    "def batch2Train(voc, batch):\n",
    "    batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    \n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    \n",
    "    for p in batch:\n",
    "        input_batch.append(p[0])\n",
    "        output_batch.append(p[1])\n",
    "        \n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    \n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347f3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81185835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94477bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402f80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3caa9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802dc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf9e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e02e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae88b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e888d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
