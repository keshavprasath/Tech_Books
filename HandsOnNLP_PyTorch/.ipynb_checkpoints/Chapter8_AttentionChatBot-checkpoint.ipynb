{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c76883",
   "metadata": {},
   "source": [
    "# Chapter 8: Building a Chatbot Using Attention-Based Neural Networks (Notes)\n",
    "\n",
    "- chatbot logic similar to seq2seq modeling but now rather, we r training it to respond based on the input sequence.\n",
    "- adding attention\n",
    "- ability to learn where in the input to took to obtain information needed than using whole seq\n",
    "\n",
    "## Theory of Attention with Neural Networks\n",
    "\n",
    "- in prev seq2seq modeling, we have an input then an encoder which obtains the hidden state and then a decoder that interprets the hidden state to produce an output\n",
    "- however, decoding HS is not efficient as HS contains all of the sentence information (excessive)\n",
    "- just parts of the sentence is relevant is prediction\n",
    "- attention makes the model look at only relevant parts to make prediction, more efficient and accurate\n",
    "\n",
    "## Comparing local and global attention\n",
    "- 2 main types of attention mechanism: local and global\n",
    "\n",
    "### local:\n",
    "- model only knows a few HS from encoder\n",
    "- basically local weights calculated from a few states based on whats needed\n",
    "- need aligned position Pt from final HS which tells which HS to be looking at for pred\n",
    "- we then calculate local weight and apply that to our HS to find context vector\n",
    "- this weight will tell us to play attention to the more relevant features and less so the others\n",
    "- this is passed into the decoder to make preds (before we send in final HS which has alot ofinformation)\n",
    "\n",
    "### global:\n",
    "- similar but now global weights calculated from all states unlike local\n",
    "- allows our model to look at any given part of the input sentence that it considers relevant\n",
    "- global attention framework is that it is essentially learning a mask that only allows through hidden states that are relevant to our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bcc58",
   "metadata": {},
   "source": [
    "# Building a ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552b04e",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- dialog data is good for chatbots, as it trains the model to respond like how humans would\n",
    "- here we are using movie script data instead \n",
    "- transform a script of n lines into n-1 pairs of input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1e1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import absolute_import\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2fd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4432c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(\"Data\", \"formatted_movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbdd73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(datafile, 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "for line in lines[:3]:\n",
    "    print(str(line) + '\\n')\n",
    "    \n",
    "'''\n",
    "call and response halves of each line are separated by a tab delimiter (/t) \n",
    "and that each of our lines is separated by a new line delimiter (/n).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e24ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0 \n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1        \n",
    "        \n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        words_to_keep = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "\n",
    "        print('Words to Keep: {} / {} = {:.2%}'.format(\n",
    "            len(words_to_keep), len(self.word2index), len(words_to_keep) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef6f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf39730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0f6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
