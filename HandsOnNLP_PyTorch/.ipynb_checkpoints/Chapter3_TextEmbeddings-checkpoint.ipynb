{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f5a1f3-6768-4643-8d1c-2605998536bb",
   "metadata": {},
   "source": [
    "# Chapter 3: NLP & Text Embeddings\n",
    "\n",
    "## Representation of Text Data\n",
    "\n",
    "- Various methods exist for representing text data in numerical form, often called embeddings.\n",
    "- Bag of Words (BoW) is a simple method that creates vectors where each element represents a word's frequency in a document.\n",
    "- However, BoW only counts the occurrences of words, while embeddings can capture the semantic meaning of words in a numerical form.\n",
    "- In this chapter, we'll explore text embeddings using the continuous Bag of Words (CBOW) model, and other methods like n-grams, tagging, chunking, tokenization, and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "## Embeddings for NLP\n",
    "\n",
    "- In natural language processing (NLP), pieces of language (words, phrases) can be represented as high-dimensional vectors, known as embeddings.\n",
    "- Embeddings are vectors of length 'n', which represent the word's vector in 'n'-dimensional space.\n",
    "- Generally, embeddings are of much lower dimensionality than the Bag of Words representation.\n",
    "- The Bag of Words representation takes into account the entire corpus, resulting in large and sparse vectors if the corpus is big.\n",
    "- Embeddings, on the other hand, typically do not exceed a few hundred dimensions, are densely packed with information, and each dimension contributes meaningfully to the representation.\n",
    "- Hence, embeddings often provide superior quality representations and are better suited for deep learning models.\n",
    "\n",
    "## GLoVE\n",
    "\n",
    "- Global Vectors (GloVe) is a pre-calculated set of word embeddings derived from a large corpus of NLP data.\n",
    "- These embeddings are trained on a word co-occurrence matrix, under the assumption that words appearing together are more likely to share a similar meaning.\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "- Cosine similarity is a measure used to determine how similar two vectors are.\n",
    "- If the angle between two vectors in an 'n'-dimensional space is 0 degrees, they are considered identical and their cosine similarity is 1.\n",
    "- High cosine similarity values indicate that two vectors are similar, even if they are not identical.\n",
    "\n",
    "## Operations on Embeddings\n",
    "\n",
    "- Mathematical operations can be performed on embeddings to capture semantic relations (e.g., Queen - Woman + Man equals King).\n",
    "- While pre-calculated embeddings such as GloVe are useful, it is also possible to generate our own embeddings, which may be beneficial when working with a unique corpus.\n",
    "\n",
    "\n",
    "### Exploring Continuous Bag of Words (CBOW)\n",
    "\n",
    "- Continuous Bag of Words (CBOW) is a part of the Word2Vec model developed by Google.\n",
    "- The Word2Vec model consists of two main components:\n",
    "    - **CBOW**: Predicts a target word in a document given the surrounding words (known as context words).\n",
    "    - **Skip-gram**: Predicts the surrounding words given a target word (opposite of CBOW).\n",
    "- For our CBOW model, we'll use a window of length 2. This means for our model's input/output pairs (X, y), we use ([n-2, n-1, n+1, n+2], n) where 'n' is our target word being predicted.\n",
    "\n",
    "### N-gram Language Models (LM)\n",
    "\n",
    "- N-gram models help understand how language can be formed (unigrams, bigrams, trigrams, etc.)\n",
    "- The meaning of a sentence is also influenced by the words around it. N-gram models capture the order of the words in a sentence, not just the frequency.\n",
    "- However, as we use larger 'n' values (e.g., bigrams, trigrams), the feature space can become extremely large quickly.\n",
    "- A bigram language model calculates the probability of a word occurring, given the word that appears before it.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- Tokenization is a pre-processing step used to split sentences into smaller parts, such as individual words or smaller documents.\n",
    "\n",
    "#### Stopwords\n",
    "\n",
    "- While some NLP tasks require stopwords (common words like 'the', 'is', 'and'), others do not. For example, in sentiment analysis of a film review, stopwords may not contribute significantly to the overall meaning.\n",
    "- Removing stopwords can reduce the feature space size, which in turn can reduce the time it takes for our models to train.\n",
    "\n",
    "### Tagging, Chunking & Parts of Speech (PoS)\n",
    "\n",
    "- Tagging, chunking, and PoS are used to capture the structure of a sentence, as different words can have different functions within a sentence.\n",
    "- These aspects and their relationships to one another can be incorporated into our models.\n",
    "\n",
    "#### Tagging\n",
    "\n",
    "- Tagging is the process of assigning PoS tags to various words in a sentence.\n",
    "- Using pre-trained PoS taggers is beneficial because they consider contextual dependencies, rather than acting like a simple lookup table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87768dcb",
   "metadata": {},
   "source": [
    "# Code Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b4c433-6093-4cd8-aaa0-16e6818bed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725d0b2a-47ba-4bc4-9925-90f97b065a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_loc = '/home/kprasath/DS/NLP/Resources/GloVe/glove.6B.50d.txt'\n",
    "\n",
    "def loadGlove(path):\n",
    "    file = open(path,'r')\n",
    "    model = {}\n",
    "    for l in file:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        value = np.array([float(val) for val in line[1:]])\n",
    "        model[word] = value\n",
    "    return model\n",
    "\n",
    "glove = loadGlove(glove_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001bbf99-0276-4845-b46c-193c72be830d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5897  , -0.55043 , -1.0106  ,  0.41226 ,  0.57348 ,  0.23464 ,\n",
       "       -0.35773 , -1.78    ,  0.10745 ,  0.74913 ,  0.45013 ,  1.0351  ,\n",
       "        0.48348 ,  0.47954 ,  0.51908 , -0.15053 ,  0.32474 ,  1.0789  ,\n",
       "       -0.90894 ,  0.42943 , -0.56388 ,  0.69961 ,  0.13501 ,  0.16557 ,\n",
       "       -0.063592,  0.35435 ,  0.42819 ,  0.1536  , -0.47018 , -1.0935  ,\n",
       "        1.361   , -0.80821 , -0.674   ,  1.2606  ,  0.29554 ,  1.0835  ,\n",
       "        0.2444  , -1.1877  , -0.60203 , -0.068315,  0.66256 ,  0.45336 ,\n",
       "       -1.0178  ,  0.68267 , -0.20788 , -0.73393 ,  1.2597  ,  0.15425 ,\n",
       "       -0.93256 , -0.15025 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "468b200e-4aa1-4663-9749-ced8d63651ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37487816]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove['jasmine'].reshape(1, -1), glove['princess'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e122b-6e01-4270-a807-bceef4606ec4",
   "metadata": {},
   "source": [
    "### Building CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1403d47a-cd1b-4939-ac59-0ac78bee3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be77c1ff-1589-43a6-9ffe-626f40e91468",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"How that personage haunted my dreams, I need scarcely tell you. On\n",
    "stormy nights, when the wind shook the four corners of the house and\n",
    "the surf roared along the cove and up the cliffs, I would see him in a\n",
    "thousand forms, and with a thousand diabolical expressions. Now the leg\n",
    "would be cut off at the knee, now at the hip, now he was a monstrous\n",
    "kind of a creature who had never had but the one leg, and that in the\n",
    "middle of his body. To see him leap and run and pursue me over hedge and\n",
    "ditch was the worst of nightmares. And altogether I paid pretty dear for\n",
    "my monthly fourpenny piece, in the shape of these abominable fancies\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961c4efa-36b5-4b78-85e1-bbe2bae3aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(',','').replace('.','').lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d07aec-35a2-4688-b284-4669857cf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set(text)\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "#use a set instead of a list as we are only concerned with the unique words within our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca97e342-c71c-45cb-acfb-d95643f4cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "inverse_word_dict = {}\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    word_dict[word] = i\n",
    "    inverse_word_dict[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4854d6a-61d2-4251-82f5-1dc29fb4ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(2, len(text) -2):\n",
    "    sentence = [text[i-2],text[i-1],text[i+1], text[i+2]]\n",
    "    target = text[i]\n",
    "    data.append((sentence, target))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8df530-6d50-44e6-abff-17fec93a0614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['haunted', 'my', 'i', 'need'], 'dreams')\n"
     ]
    }
   ],
   "source": [
    "print(data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d3f48-2716-41cf-85d7-18426c3066d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While higher-dimensional embeddings can lead to a more detailed representation of the words, the feature space also becomes sparser\n",
    "#which means high-dimensional embeddings are only appropriate for large corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48dc24f0-e02b-46c7-94d9-2860f3d04847",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf661e6-1f9a-49af-a698-b7781e4fcee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, corpus_length, embedding_dim):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embeddings = nn.Embedding(corpus_length, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim,64)\n",
    "        self.linear2 = nn.Linear(64,corpus_length)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "    \n",
    "    def get_word_embedding(self,word):\n",
    "        word = torch.LongTensor([word_dict[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "365c452e-f35e-4181-981f-50b7439b1c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50, 51,  5, 18])\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "\n",
    "model = CBOW(corpus_length, embedding_length)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def make_sentence_vector(sentence, word_dict):\n",
    "    idxs = [word_dict[w] for w in sentence]\n",
    "    return torch.tensor(idxs, dtype = torch.long)\n",
    "print(make_sentence_vector(['stormy','nights','when','the'], word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c92512-602a-4b63-927c-8c8c65a1fcfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 544.8351440429688\n",
      "Epoch: 1, Loss: 480.48858642578125\n",
      "Epoch: 2, Loss: 435.4610595703125\n",
      "Epoch: 3, Loss: 395.34063720703125\n",
      "Epoch: 4, Loss: 355.692626953125\n",
      "Epoch: 5, Loss: 316.1547546386719\n",
      "Epoch: 6, Loss: 276.779541015625\n",
      "Epoch: 7, Loss: 238.30360412597656\n",
      "Epoch: 8, Loss: 201.53378295898438\n",
      "Epoch: 9, Loss: 167.13568115234375\n",
      "Epoch: 10, Loss: 136.44906616210938\n",
      "Epoch: 11, Loss: 110.07818603515625\n",
      "Epoch: 12, Loss: 88.14295959472656\n",
      "Epoch: 13, Loss: 70.43659973144531\n",
      "Epoch: 14, Loss: 56.51333999633789\n",
      "Epoch: 15, Loss: 45.71113586425781\n",
      "Epoch: 16, Loss: 37.46113586425781\n",
      "Epoch: 17, Loss: 31.1269474029541\n",
      "Epoch: 18, Loss: 26.23110008239746\n",
      "Epoch: 19, Loss: 22.432907104492188\n",
      "Epoch: 20, Loss: 19.406402587890625\n",
      "Epoch: 21, Loss: 17.020523071289062\n",
      "Epoch: 22, Loss: 15.0863037109375\n",
      "Epoch: 23, Loss: 13.500479698181152\n",
      "Epoch: 24, Loss: 12.192682266235352\n",
      "Epoch: 25, Loss: 11.087519645690918\n",
      "Epoch: 26, Loss: 10.160371780395508\n",
      "Epoch: 27, Loss: 9.360690116882324\n",
      "Epoch: 28, Loss: 8.668476104736328\n",
      "Epoch: 29, Loss: 8.065563201904297\n",
      "Epoch: 30, Loss: 7.5345458984375\n",
      "Epoch: 31, Loss: 7.06592321395874\n",
      "Epoch: 32, Loss: 6.647702217102051\n",
      "Epoch: 33, Loss: 6.2716898918151855\n",
      "Epoch: 34, Loss: 5.935548782348633\n",
      "Epoch: 35, Loss: 5.629001140594482\n",
      "Epoch: 36, Loss: 5.35107946395874\n",
      "Epoch: 37, Loss: 5.098361968994141\n",
      "Epoch: 38, Loss: 4.865406036376953\n",
      "Epoch: 39, Loss: 4.653190612792969\n",
      "Epoch: 40, Loss: 4.456249237060547\n",
      "Epoch: 41, Loss: 4.274328231811523\n",
      "Epoch: 42, Loss: 4.106461048126221\n",
      "Epoch: 43, Loss: 3.9501678943634033\n",
      "Epoch: 44, Loss: 3.8033056259155273\n",
      "Epoch: 45, Loss: 3.6686158180236816\n",
      "Epoch: 46, Loss: 3.540703773498535\n",
      "Epoch: 47, Loss: 3.4210777282714844\n",
      "Epoch: 48, Loss: 3.3095927238464355\n",
      "Epoch: 49, Loss: 3.204206943511963\n",
      "Epoch: 50, Loss: 3.1045117378234863\n",
      "Epoch: 51, Loss: 3.0113253593444824\n",
      "Epoch: 52, Loss: 2.9223155975341797\n",
      "Epoch: 53, Loss: 2.8385169506073\n",
      "Epoch: 54, Loss: 2.7590951919555664\n",
      "Epoch: 55, Loss: 2.683626890182495\n",
      "Epoch: 56, Loss: 2.611649990081787\n",
      "Epoch: 57, Loss: 2.5436317920684814\n",
      "Epoch: 58, Loss: 2.4786791801452637\n",
      "Epoch: 59, Loss: 2.41664457321167\n",
      "Epoch: 60, Loss: 2.3578197956085205\n",
      "Epoch: 61, Loss: 2.301130533218384\n",
      "Epoch: 62, Loss: 2.247380018234253\n",
      "Epoch: 63, Loss: 2.1955628395080566\n",
      "Epoch: 64, Loss: 2.1461634635925293\n",
      "Epoch: 65, Loss: 2.0985751152038574\n",
      "Epoch: 66, Loss: 2.0531139373779297\n",
      "Epoch: 67, Loss: 2.0094659328460693\n",
      "Epoch: 68, Loss: 1.967372179031372\n",
      "Epoch: 69, Loss: 1.9269238710403442\n",
      "Epoch: 70, Loss: 1.8879854679107666\n",
      "Epoch: 71, Loss: 1.8506739139556885\n",
      "Epoch: 72, Loss: 1.814449429512024\n",
      "Epoch: 73, Loss: 1.779654622077942\n",
      "Epoch: 74, Loss: 1.746131420135498\n",
      "Epoch: 75, Loss: 1.71364164352417\n",
      "Epoch: 76, Loss: 1.6823790073394775\n",
      "Epoch: 77, Loss: 1.6521133184432983\n",
      "Epoch: 78, Loss: 1.6227842569351196\n",
      "Epoch: 79, Loss: 1.5943920612335205\n",
      "Epoch: 80, Loss: 1.5671093463897705\n",
      "Epoch: 81, Loss: 1.5404822826385498\n",
      "Epoch: 82, Loss: 1.514797329902649\n",
      "Epoch: 83, Loss: 1.4898372888565063\n",
      "Epoch: 84, Loss: 1.4656453132629395\n",
      "Epoch: 85, Loss: 1.4421700239181519\n",
      "Epoch: 86, Loss: 1.4193025827407837\n",
      "Epoch: 87, Loss: 1.3973640203475952\n",
      "Epoch: 88, Loss: 1.3757270574569702\n",
      "Epoch: 89, Loss: 1.3549139499664307\n",
      "Epoch: 90, Loss: 1.3346890211105347\n",
      "Epoch: 91, Loss: 1.314825177192688\n",
      "Epoch: 92, Loss: 1.2956284284591675\n",
      "Epoch: 93, Loss: 1.2771351337432861\n",
      "Epoch: 94, Loss: 1.2587025165557861\n",
      "Epoch: 95, Loss: 1.2410662174224854\n",
      "Epoch: 96, Loss: 1.2238199710845947\n",
      "Epoch: 97, Loss: 1.2069016695022583\n",
      "Epoch: 98, Loss: 1.1906355619430542\n",
      "Epoch: 99, Loss: 1.1745802164077759\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for sentence, target in data:\n",
    "        model.zero_grad()\n",
    "        sentence_vector = make_sentence_vector(sentence, word_dict)  \n",
    "        log_probs = model(sentence_vector)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_dict[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data\n",
    "    print('Epoch: '+str(epoch)+', Loss: ' + str(epoch_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67a7d84-8988-4c23-8654-6f5de77f9ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding Words: ['to', 'see']\n",
      "\n",
      "Predicted Word: him\n",
      "\n",
      "Following Words: ['leap', 'and']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_predicted_result(input, inverse_word_dict):\n",
    "    index = np.argmax(input)\n",
    "    return inverse_word_dict[index]\n",
    "\n",
    "def predict_sentence(sentence):\n",
    "    sentence_split = sentence.replace('.','').lower().split()\n",
    "    sentence_vector = make_sentence_vector(sentence_split, word_dict)\n",
    "    prediction_array = model(sentence_vector).data.numpy()\n",
    "    print('Preceding Words: {}\\n'.format(sentence_split[:2]))\n",
    "    print('Predicted Word: {}\\n'.format(get_predicted_result(prediction_array[0], inverse_word_dict)))\n",
    "    print('Following Words: {}\\n'.format(sentence_split[2:]))\n",
    "\n",
    "predict_sentence('to see leap and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7edb405-2bae-4d3b-9af0-444e8ae8c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0962,  1.9593,  0.0110,  0.1595,  0.7783,  0.9081, -0.5268, -1.8456,\n",
      "         -1.0762, -0.8705,  0.4062,  0.1166, -0.1222,  1.3338, -0.3573,  0.0959,\n",
      "          0.8397, -1.2704,  1.1369, -1.8897]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_word_embedding('leap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b604a3ff-d256-454e-be96-2ff4f8f0556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90fc26c3-4eb9-4e3f-a3a2-96b1cacf99e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'single', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a single sentence.'\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "#here '.' is also included, depends on what you need it for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ec3f5d1-61c2-4f8e-bd16-9edb1100ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'single', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "no_punctuation = [word.lower() for word in tokens if word.isalpha()]\n",
    "print(no_punctuation)\n",
    "\n",
    "#removes puncutuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cb90713-b428-4a68-a88f-ac72d31b4753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence.', 'This is the second sentence.', 'A document contains many sentences.']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenisation \n",
    "text = \"This is the first sentence. This is the second sentence. A document contains many sentences.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15da599-6222-4d18-a0fd-6fdb3b140463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'first', 'sentence', '.'], ['This', 'is', 'the', 'second', 'sentence', '.'], ['A', 'document', 'contains', 'many', 'sentences', '.']]\n"
     ]
    }
   ],
   "source": [
    "print([word_tokenize(sentence) for sentence in sent_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e47876ca-3c55-484f-ac21-26c8810ca8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "#stopwords stuff, already known\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2849b90-d1ec-4500-a440-04688d190886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'single', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a single sentence.'\n",
    "tokens = [token for token in word_tokenize(text) if token not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840a7b4",
   "metadata": {},
   "source": [
    "### Tagging, Chunking & PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92152aeb-ac18-4d3b-b4d7-be1824480438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a811f55-bc34-4423-934b-532ac48377ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('sleeping', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('bed', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The big dog is sleeping on the bed\"\n",
    "token = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "813a6651-047b-4838-87c9-f94a557c13ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25575177-083d-4b0e-b73f-cf74aa086bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9099c1cc-d662-4788-ad23-16b6a8feae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = ('NP: {<DT>?<JJ>*<NN>}')\n",
    "REchunkParser = nltk.RegexpParser(expression)\n",
    "tree = REchunkParser.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d49a420-7acb-4cb3-afea-1fa78782a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717a51e",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebbaeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b1d75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "\n",
    "emma_sentences = []\n",
    "emma_word_set = []\n",
    "\n",
    "for sentence in emma:\n",
    "    emma_sentences.append([word.lower() for word in sentence if word.isalpha()])\n",
    "    for word in sentence:\n",
    "        if word.isalpha():\n",
    "            emma_word_set.append(word.lower())\n",
    "\n",
    "emma_word_set = set(emma_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a68ccb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024390243902439025"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def TermFreq(document, word):\n",
    "    doc_length = len(document)\n",
    "    occurances = len([w for w in document if w == word])\n",
    "    return occurances / doc_length\n",
    "\n",
    "TermFreq(emma_sentences[5], 'ago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "215f22e6-f443-4e80-abec-902f1d6a3690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_DF_dict():\n",
    "    output = {}\n",
    "    for word in emma_word_set:\n",
    "        output[word] = 0\n",
    "        for doc in emma_sentences:\n",
    "            if word in doc:\n",
    "                output[word] += 1\n",
    "    return output\n",
    "        \n",
    "df_dict = build_DF_dict()\n",
    "\n",
    "df_dict['ago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4bbfb2b-559b-4ec5-9509-cb48ce01bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.454673404991123"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def InverseDocumentFrequency(word):\n",
    "    N = len(emma_sentences)\n",
    "    try:\n",
    "        df = df_dict[word] + 1\n",
    "    except:\n",
    "        df = 1\n",
    "    return np.log(N/df)\n",
    "\n",
    "InverseDocumentFrequency('ago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ce68b9c-22ff-4c07-a9ac-144da405a2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ago - 0.13304081475588106\n",
      "indistinct - 0.2014154581926258\n"
     ]
    }
   ],
   "source": [
    "def TFIDF(doc, word):\n",
    "    tf = TermFreq(doc, word)\n",
    "    idf = InverseDocumentFrequency(word)\n",
    "    return tf*idf\n",
    "\n",
    "print('ago - ' + str(TFIDF(emma_sentences[5],'ago')))\n",
    "print('indistinct - ' + str(TFIDF(emma_sentences[5],'indistinct')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c5ad10b-9b70-424b-a183-317bb766743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.32575634e-01  3.16596488e-01 -1.80050732e-01 -3.82070951e-01\n",
      "   4.98493527e-01  5.33804805e-01 -5.46517073e-01  9.12476195e-02\n",
      "  -1.31538483e-01 -2.71967805e-02  2.99867317e-02  2.64278024e-02\n",
      "  -2.06519756e-01 -1.54796634e-01  4.28036366e-01 -5.74977317e-02\n",
      "  -2.65928778e-01  1.60373902e-02 -2.84913561e-01 -2.01252268e-01\n",
      "  -5.96390732e-02  5.72458220e-01  2.06195927e-01 -1.54312293e-01\n",
      "   2.52049805e-01 -1.64638200e+00 -3.42686049e-01  1.02592522e-01\n",
      "   1.42848000e-01 -1.09779902e-01  2.89345488e+00  7.36985634e-02\n",
      "  -3.73648780e-03 -2.76292784e-01  1.50580049e-01  9.80399951e-02\n",
      "   2.24408780e-03  2.83664024e-01  3.92979024e-02 -2.98091634e-01\n",
      "  -1.17309171e-01  2.08815776e-01  6.89953902e-03  2.92777244e-02\n",
      "   5.54180122e-02 -2.20519707e-01 -2.82007805e-01 -4.34917439e-01\n",
      "  -9.69051537e-02 -1.67569878e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = []\n",
    "\n",
    "for word in emma_sentences[5]:\n",
    "    embeddings.append(glove[word])\n",
    "\n",
    "mean_embedding = np.mean(embeddings, axis = 0).reshape(1, -1)\n",
    "\n",
    "print(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a18ce9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03384888  0.04561131 -0.02508487 -0.05546237  0.0651311   0.07019455\n",
      "  -0.06298467  0.02670422 -0.01072827 -0.00508234  0.00517652  0.00817101\n",
      "  -0.01604324 -0.01483237  0.04946372 -0.01076198 -0.05021479  0.00040191\n",
      "  -0.01920397 -0.01341318 -0.01123547  0.08492142  0.02142466 -0.01588025\n",
      "   0.04405683 -0.17856836 -0.03999452  0.01601948  0.02088402 -0.01340125\n",
      "   0.2829529   0.00694315  0.00485215 -0.02633143  0.01534283  0.01608815\n",
      "   0.00316191  0.03238881  0.0082704  -0.04192922 -0.0058766   0.01992215\n",
      "  -0.00304265 -0.00353939  0.01174628 -0.03416807 -0.02939215 -0.06798914\n",
      "  -0.00774682 -0.01807456]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "for word in emma_sentences[5]:\n",
    "    tfidf = TFIDF(emma_sentences[5], word)\n",
    "    embeddings.append(glove[word]* tfidf) \n",
    "    \n",
    "tfidf_weighted_embedding = np.mean(embeddings, axis = 0).reshape(1, -1)\n",
    "\n",
    "print(tfidf_weighted_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51dc934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.986523]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(mean_embedding, tfidf_weighted_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
