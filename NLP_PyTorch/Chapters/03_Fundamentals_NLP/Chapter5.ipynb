{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cd61b1",
   "metadata": {},
   "source": [
    "# RNNs & Sentiment Analysis (NOTES)\n",
    "\n",
    "\n",
    "- RecurrentNN variation of the FF-NNs\n",
    "- used when task can be represented as a sequence\n",
    "- sentence is a seq of words\n",
    "- RNN takes whole sequence of vectors as input (CNN takes single vector)\n",
    "- if each word in document is a vector embedd, then whole document can be represented as order 3 tensor\n",
    "- LSTM (a more sophisticated RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e9f9d",
   "metadata": {},
   "source": [
    "### Theory of Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- RNNs are structured with recurrent layers, similar to standard feedforward neural networks (NNs).\n",
    "- They incorporate a hidden recurrent state that gets updated at each step during sequence processing.\n",
    "- At the start of processing any sequence, the model is initialized with a one-dimensional vector representing the hidden state.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "- Each word in the sequence is fed into the model, leading to an update in the hidden state (HS).\n",
    "- This process continues until all words have been processed, generating a final hidden state vector.\n",
    "- This final HS vector is then fed into a fully connected layer to yield the final class prediction.\n",
    "- Activation functions, such as tanh, can be employed to constrain the hidden state values between -1 and 1.\n",
    "- During learning, weights are updated at each step, and the loss is computed during backpropagation.\n",
    "- For tasks such as sequence-to-sequence translation in NLP, hidden state values from each layer can be utilized instead of only the final HS.\n",
    "\n",
    "### RNN for Sentiment Analysis\n",
    "\n",
    "- Sentiment analysis becomes a binary classification task (i.e., positive or negative) when using RNNs.\n",
    "\n",
    "### Potential Challenges and Solutions\n",
    "\n",
    "- One of the challenges with RNNs is the occurrence of exploding or vanishing gradients due to the recursive layers, which can cause instability in the network.\n",
    "- Solutions include:\n",
    "  - Implementing gradient clipping to limit the gradients from becoming excessively large. This involves introducing a hyperparameter C to establish an upper limit.\n",
    "  - Reducing the input sequence length. Since a shorter sequence means fewer iterations, the maximum sequence length can be chosen as a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775ee5f",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Flaws of RNN\n",
    "- hard to retain information long term (cant capture long-term sentence dependency)\n",
    "- poor at capturing context of word within sentence (lacing context dependency, due to how its trained)\n",
    "- unable to predict things that came early on in the sentence due to it being trained in one directional\n",
    "\n",
    "LSTM can combat these issues\n",
    "- a LSTM is a more sophisticated RNN and contains 2 extra propertises (an update gate and a forget gate)\n",
    "- these 2 additions, make it easier to learn long term dependencies\n",
    "- in context of Sentiment Analysis, LSTM will remember important information and leave our irrelevant ones, preventing the irrelevent features to dilute important information, maintaining long term dependencies\n",
    "\n",
    "- LSTM has a similar strcuture to RNN with the recursive hidden state but the LSTM cell is more complex\n",
    "- it has a series of gates that allow for the additional calculations. lets break it down\n",
    "\n",
    "#### forget gate\n",
    "- learns which elements of the sequence to forget\n",
    "- the previous hidden state (h. t-1) and the latest input step (x1) and concat. together and pass through a matrix of learned weights on the forget gate\n",
    "a sigmoid function the bounds the value between 0 and 1\n",
    "- This resulting matrix, ft, is multiplied pointwise by the cell state from the previous step, ct-1. \n",
    "- This effectively applies a mask to the previous cell state so that only the relevant information from the previous cell state is brought forward.\n",
    "\n",
    "#### input gate\n",
    "- takes in concat. input and pass it into a sigmoid function to bound between 0 and 1\n",
    "\n",
    "#### output gate\n",
    "- calculates final output out of the LSTM cell\n",
    "- learned para on output gate control which elemets of the previous HS and current output and combined to then carry forward to the next stage\n",
    "\n",
    "- in one forward pass, we iterate through the model, init hidden state, cell stae and update at each stem\n",
    "- backprop used to calculate gradients relative to loss, to know which direction to update our para\n",
    "- LSTM has more computations than RNN, so more complex computation graph \n",
    "- and backprop calculation for gradient will also take longer\n",
    "- but despite longer time, LSTM offers signifant improve in performance compared to RNN\n",
    "- this is because the 3 gates give the model ability to determine which elements of the input should be used to udpate the hidden state etc\n",
    "- which means model is better at forming long term dependencies and retain information from prev steps\n",
    "\n",
    "### Bidirectional LSTMs\n",
    "- modifed LSTM that considers both the words before and adter it at each step within sequence\n",
    "- LSTMMs process seq in regular order and reserve order simultaniosly, maintaining 2 hidden states\n",
    "- this allows for the context of any given word within seq can be better captured\n",
    "- Bidirectional LSTM offer improved performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f7754",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e7765",
   "metadata": {},
   "source": [
    "## Building A Sentiment Analyzer using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6947c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ba0684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn = 3000\\nData comes from 3 diff sources - film, product and location reviews\\nlabel = 0,1 at 50/50 split\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "n = 3000\n",
    "Data comes from 3 diff sources - film, product and location reviews\n",
    "label = 0,1 at 50/50 split\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fb4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentiment.txt\") as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "data = pd.DataFrame([review.split('\\t') for review in reviews.split('\\n')])\n",
    "data.columns = ['Review','Sentiment']\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8639517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>The warmth it generates is in contrast to its ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>Cute, quaint, simple, honest.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>I've owned this phone for 7 months now and can...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>It presents a idyllic yet serious portrayal of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>Highly recommended.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>When I'm on this side of town, this will defin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Give this one a look.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>The last 3 times I had lunch here has been bad.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Excellent cast, story line, performances.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>This one works and was priced right.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Everything from acting to cinematography was s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>Phone falls out easily.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>I keep watching it over and over.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>I believe every one should see this movie as I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Sentiment\n",
       "816   The warmth it generates is in contrast to its ...         1\n",
       "1720                      Cute, quaint, simple, honest.         1\n",
       "2026  I've owned this phone for 7 months now and can...         1\n",
       "952   It presents a idyllic yet serious portrayal of...         1\n",
       "1016                                Highly recommended.         1\n",
       "1755  When I'm on this side of town, this will defin...         1\n",
       "85                              Give this one a look.           1\n",
       "1454    The last 3 times I had lunch here has been bad.         0\n",
       "56          Excellent cast, story line, performances.           1\n",
       "2297               This one works and was priced right.         1\n",
       "362   Everything from acting to cinematography was s...         1\n",
       "2987                            Phone falls out easily.         0\n",
       "981                 I keep watching it over and over.           1\n",
       "400   A good commentary of today's love and undoubte...         1\n",
       "649   I believe every one should see this movie as I...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1822c3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'warmth',\n",
       " 'it',\n",
       " 'generates',\n",
       " 'is',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'its',\n",
       " 'austere',\n",
       " 'backdrop']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proprocessing Step\n",
    "\n",
    "def split_words_reviews(data):\n",
    "    text = list(data['Review'].values)\n",
    "    clean_text = []\n",
    "    for t in text:\n",
    "        clean_text.append(t.translate(str.maketrans('', '', punctuation)).lower().rstrip()) #lowers\n",
    "    tokenized = [word_tokenize(x) for x in clean_text] # tokenise\n",
    "    all_text = []\n",
    "    for tokens in tokenized:\n",
    "        for t in tokens:\n",
    "            all_text.append(t)\n",
    "    return tokenized, set(all_text) #set for unique word count (corpus)\n",
    "\n",
    "reviews, vocab = split_words_reviews(data)\n",
    "\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8f90ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'abound',\n",
       " 2: 'chalkboard',\n",
       " 3: 'tying',\n",
       " 4: 'chains',\n",
       " 5: 'unacceptible',\n",
       " 6: 'mortified',\n",
       " 7: 'shallow',\n",
       " 8: 'london',\n",
       " 9: 'sensitivities',\n",
       " 10: 'rpg',\n",
       " 11: 'gristle',\n",
       " 12: 'gels',\n",
       " 13: 'sidelined',\n",
       " 14: 'burgers',\n",
       " 15: 'rushed',\n",
       " 16: 'candle',\n",
       " 17: '110',\n",
       " 18: 'bathrooms',\n",
       " 19: 'subtle',\n",
       " 20: 'primal',\n",
       " 21: 'moved',\n",
       " 22: 'delete',\n",
       " 23: 'mouse',\n",
       " 24: 'window',\n",
       " 25: 'most',\n",
       " 26: 'improvisation',\n",
       " 27: 'croutons',\n",
       " 28: 'engaging',\n",
       " 29: 'theory',\n",
       " 30: 'steamboat',\n",
       " 31: 'movieit',\n",
       " 32: 'surrounding',\n",
       " 33: 'six',\n",
       " 34: 'watsons',\n",
       " 35: 'union',\n",
       " 36: 'pissd',\n",
       " 37: 'chilly',\n",
       " 38: 'finish',\n",
       " 39: 'utter',\n",
       " 40: 'filmmaking',\n",
       " 41: 'recommending',\n",
       " 42: 'livingworking',\n",
       " 43: 'z',\n",
       " 44: 'upper',\n",
       " 45: 'beep',\n",
       " 46: 'portrayed',\n",
       " 47: 'jean',\n",
       " 48: 'muddled',\n",
       " 49: 'good',\n",
       " 50: 'grimes',\n",
       " 51: 'admins',\n",
       " 52: 'card',\n",
       " 53: 'burger',\n",
       " 54: 'mile',\n",
       " 55: 'dangerous',\n",
       " 56: 'reactions',\n",
       " 57: 'itdefinitely',\n",
       " 58: 'ears',\n",
       " 59: 'cutouts',\n",
       " 60: 'babysitting',\n",
       " 61: 'improved',\n",
       " 62: 'flops',\n",
       " 63: 'save',\n",
       " 64: 'reminded',\n",
       " 65: 'contacted',\n",
       " 66: 'always',\n",
       " 67: 'ticking',\n",
       " 68: 'disconnected',\n",
       " 69: 'wrotedirected',\n",
       " 70: 'ball',\n",
       " 71: 'pedestal',\n",
       " 72: 'wedding',\n",
       " 73: 'guilt',\n",
       " 74: 'rip',\n",
       " 75: 'outdoor',\n",
       " 76: 'hay',\n",
       " 77: 'promptly',\n",
       " 78: 'casino',\n",
       " 79: 'despised',\n",
       " 80: 'connected',\n",
       " 81: 'against',\n",
       " 82: 'crash',\n",
       " 83: 'rings',\n",
       " 84: 'ill',\n",
       " 85: 'casted',\n",
       " 86: 'will',\n",
       " 87: 'nothing',\n",
       " 88: 'nurse',\n",
       " 89: 'garbled',\n",
       " 90: 'generally',\n",
       " 91: 'bed',\n",
       " 92: 'sort',\n",
       " 93: 'practical',\n",
       " 94: 'structure',\n",
       " 95: 'blandly',\n",
       " 96: 'melted',\n",
       " 97: 'spring',\n",
       " 98: 'puzzlesolving',\n",
       " 99: 'attitudes',\n",
       " 100: 'tolerance',\n",
       " 101: 'expanded',\n",
       " 102: 'onion',\n",
       " 103: 'unnecessary',\n",
       " 104: 'performed',\n",
       " 105: 'booksomethats',\n",
       " 106: 'have',\n",
       " 107: 'behind',\n",
       " 108: 'stuck',\n",
       " 109: 'ladies',\n",
       " 110: 'satifying',\n",
       " 111: 'deadly',\n",
       " 112: 'unrealistic',\n",
       " 113: 'fella',\n",
       " 114: 'kitchy',\n",
       " 115: 'leather',\n",
       " 116: 'treo',\n",
       " 117: 'opening',\n",
       " 118: 'related',\n",
       " 119: 'hide',\n",
       " 120: 'cotta',\n",
       " 121: 'coal',\n",
       " 122: 'risotto',\n",
       " 123: 'hardly',\n",
       " 124: 'appetite',\n",
       " 125: 'caused',\n",
       " 126: 'modern',\n",
       " 127: 'giant',\n",
       " 128: 'absolute',\n",
       " 129: 'seuss',\n",
       " 130: 'novella',\n",
       " 131: 'below',\n",
       " 132: 'blows',\n",
       " 133: 'outta',\n",
       " 134: 'exceptional',\n",
       " 135: 'dropping',\n",
       " 136: 'concept',\n",
       " 137: 'loudglad',\n",
       " 138: 'want',\n",
       " 139: 'reversible',\n",
       " 140: 'eiko',\n",
       " 141: 'social',\n",
       " 142: 'thug',\n",
       " 143: 'eew',\n",
       " 144: 'bowl',\n",
       " 145: 'taken',\n",
       " 146: 'searched',\n",
       " 147: 'trust',\n",
       " 148: 'admiration',\n",
       " 149: '5of',\n",
       " 150: 'laughs',\n",
       " 151: 'fs',\n",
       " 152: 'hang',\n",
       " 153: 'exaggerating',\n",
       " 154: 'catchy',\n",
       " 155: 'products',\n",
       " 156: 'hated',\n",
       " 157: 'monotonous',\n",
       " 158: 'director',\n",
       " 159: 'youtube',\n",
       " 160: 'comparablypriced',\n",
       " 161: 'bullock',\n",
       " 162: 'moment',\n",
       " 163: 'docking',\n",
       " 164: 'abhor',\n",
       " 165: 'cancellation',\n",
       " 166: 'interested',\n",
       " 167: 'pics',\n",
       " 168: 'hours',\n",
       " 169: '1973',\n",
       " 170: 'weak',\n",
       " 171: 'cakeohhh',\n",
       " 172: 'transfers',\n",
       " 173: 'zombiestudents',\n",
       " 174: 'emotions',\n",
       " 175: 'hitch',\n",
       " 176: 'freezing',\n",
       " 177: 'drawing',\n",
       " 178: 'starlet',\n",
       " 179: 'impression',\n",
       " 180: 'appears',\n",
       " 181: '1948',\n",
       " 182: 'poisoning',\n",
       " 183: 'awards',\n",
       " 184: 'blare',\n",
       " 185: 'defined',\n",
       " 186: 'males',\n",
       " 187: 'enjoyed',\n",
       " 188: 'smoothies',\n",
       " 189: 'returning',\n",
       " 190: 'motor',\n",
       " 191: 'issue',\n",
       " 192: 'activated',\n",
       " 193: 'excessively',\n",
       " 194: 'crepe',\n",
       " 195: 'problemvery',\n",
       " 196: 'means',\n",
       " 197: 'become',\n",
       " 198: 'clarity',\n",
       " 199: 'keypads',\n",
       " 200: 'silent',\n",
       " 201: 'phonesmp3',\n",
       " 202: 'shops',\n",
       " 203: 'chow',\n",
       " 204: '9',\n",
       " 205: 'example',\n",
       " 206: 'culture',\n",
       " 207: 'entire',\n",
       " 208: 'lost',\n",
       " 209: 'pseudosatanic',\n",
       " 210: 'author',\n",
       " 211: 'day',\n",
       " 212: 'fliptop',\n",
       " 213: 'innocence',\n",
       " 214: 'genuine',\n",
       " 215: 'regrettably',\n",
       " 216: 'reality',\n",
       " 217: 'tooth',\n",
       " 218: 'handling',\n",
       " 219: 'shirt',\n",
       " 220: 'grilled',\n",
       " 221: 'nobu',\n",
       " 222: 'significantly',\n",
       " 223: 'served',\n",
       " 224: 'panna',\n",
       " 225: 'backdrop',\n",
       " 226: 'whoever',\n",
       " 227: 'cable',\n",
       " 228: 'probably',\n",
       " 229: 'garden',\n",
       " 230: 'salmon',\n",
       " 231: 'plug',\n",
       " 232: 'raver',\n",
       " 233: 'dreary',\n",
       " 234: 'roast',\n",
       " 235: 'sitdown',\n",
       " 236: 'sanyo',\n",
       " 237: 'allergy',\n",
       " 238: 'legs',\n",
       " 239: 'ease',\n",
       " 240: 'ms',\n",
       " 241: 'dying',\n",
       " 242: 'embassy',\n",
       " 243: 'app',\n",
       " 244: 'coverage',\n",
       " 245: 'hostess',\n",
       " 246: 'actingwise',\n",
       " 247: 'laughable',\n",
       " 248: 'iphone',\n",
       " 249: 'sat',\n",
       " 250: 'credits',\n",
       " 251: 'clip',\n",
       " 252: 'sunday',\n",
       " 253: 'you',\n",
       " 254: 'goalies',\n",
       " 255: 'scaredand',\n",
       " 256: 'jealousy',\n",
       " 257: 'every',\n",
       " 258: '8125',\n",
       " 259: 'excellentangel',\n",
       " 260: 'elaborately',\n",
       " 261: 'closeups',\n",
       " 262: 'answer',\n",
       " 263: 'chodorov',\n",
       " 264: 'call',\n",
       " 265: 'monster',\n",
       " 266: 'serious',\n",
       " 267: 'seamlessly',\n",
       " 268: 'trippy',\n",
       " 269: 'never',\n",
       " 270: 'droid',\n",
       " 271: 'vomit',\n",
       " 272: 'subverting',\n",
       " 273: '20',\n",
       " 274: 'idealogical',\n",
       " 275: 'messaging',\n",
       " 276: 'irons',\n",
       " 277: 'bluetooths',\n",
       " 278: 'evening',\n",
       " 279: 'recognizes',\n",
       " 280: 'gently',\n",
       " 281: 'student',\n",
       " 282: 'painfully',\n",
       " 283: 'tacky',\n",
       " 284: 'personally',\n",
       " 285: 'basement',\n",
       " 286: 'outrageously',\n",
       " 287: 'unmoving',\n",
       " 288: 'happy',\n",
       " 289: 'headphones',\n",
       " 290: 'dusted',\n",
       " 291: 'jeff',\n",
       " 292: '8pm',\n",
       " 293: 'woo',\n",
       " 294: 'welsh',\n",
       " 295: 'eccleston',\n",
       " 296: 'splendid',\n",
       " 297: 'debbie',\n",
       " 298: 'earlier',\n",
       " 299: 'pats',\n",
       " 300: 'bumpers',\n",
       " 301: 'tops',\n",
       " 302: 'forgetting',\n",
       " 303: 'forces',\n",
       " 304: 'dialing',\n",
       " 305: 'able',\n",
       " 306: 'strawberry',\n",
       " 307: 'tots',\n",
       " 308: 'fact',\n",
       " 309: 'starts',\n",
       " 310: 'muddy',\n",
       " 311: 'get',\n",
       " 312: 'phoenix',\n",
       " 313: 'continuation',\n",
       " 314: 'flatlined',\n",
       " 315: 'don',\n",
       " 316: 'anne',\n",
       " 317: 'plain',\n",
       " 318: 'appetizer',\n",
       " 319: 'decipher',\n",
       " 320: 'atmosphere',\n",
       " 321: 'palm',\n",
       " 322: 'push',\n",
       " 323: 'knows',\n",
       " 324: 'feels',\n",
       " 325: 'rude',\n",
       " 326: 'unacceptable',\n",
       " 327: '5320',\n",
       " 328: '1010',\n",
       " 329: 'yun',\n",
       " 330: 'intensity',\n",
       " 331: 'meteorite',\n",
       " 332: 'pita',\n",
       " 333: 'arent',\n",
       " 334: 'horse',\n",
       " 335: 'is',\n",
       " 336: 'inappropriate',\n",
       " 337: 'v115g',\n",
       " 338: 'bodes',\n",
       " 339: 'powerhouse',\n",
       " 340: 'forgettable',\n",
       " 341: 'ben',\n",
       " 342: 'role',\n",
       " 343: 'mandalay',\n",
       " 344: 'preparing',\n",
       " 345: 'dracula',\n",
       " 346: 'delicioso',\n",
       " 347: 'universal',\n",
       " 348: 'got',\n",
       " 349: 'elk',\n",
       " 350: 'watchable',\n",
       " 351: 'strip',\n",
       " 352: 'critical',\n",
       " 353: 'poetry',\n",
       " 354: 'indeed',\n",
       " 355: 'teamwork',\n",
       " 356: 'data',\n",
       " 357: 'random',\n",
       " 358: 'mixed',\n",
       " 359: 'juice',\n",
       " 360: 'summarize',\n",
       " 361: 'taylors',\n",
       " 362: 'escapism',\n",
       " 363: 'ohsomature',\n",
       " 364: 'grill',\n",
       " 365: 'kitchen',\n",
       " 366: 'major',\n",
       " 367: 'navigate',\n",
       " 368: 'does',\n",
       " 369: 'performance',\n",
       " 370: 'hair',\n",
       " 371: 'seated',\n",
       " 372: 'hole',\n",
       " 373: 'michael',\n",
       " 374: 'lightweight',\n",
       " 375: 'incomprehensible',\n",
       " 376: 'starter',\n",
       " 377: 'waiter',\n",
       " 378: 'ache',\n",
       " 379: 'happening',\n",
       " 380: 'youthful',\n",
       " 381: 'indescribably',\n",
       " 382: 'circumstances',\n",
       " 383: 'gere',\n",
       " 384: 'laughing',\n",
       " 385: 'arepas',\n",
       " 386: 'gore',\n",
       " 387: 'burrittos',\n",
       " 388: 'government',\n",
       " 389: 'sensitive',\n",
       " 390: 'actorsan',\n",
       " 391: 'elegant',\n",
       " 392: 'camp',\n",
       " 393: 'regardless',\n",
       " 394: 'completed',\n",
       " 395: 'lettuce',\n",
       " 396: 'use',\n",
       " 397: 'vanilla',\n",
       " 398: 'lovable',\n",
       " 399: 'race',\n",
       " 400: 'existing',\n",
       " 401: 'yama',\n",
       " 402: 'sea',\n",
       " 403: 'mention',\n",
       " 404: 'quit',\n",
       " 405: 'fry',\n",
       " 406: 'trip',\n",
       " 407: 'memories',\n",
       " 408: 'grasp',\n",
       " 409: 'zombiez',\n",
       " 410: 'focus',\n",
       " 411: 'dustpan',\n",
       " 412: 'bend',\n",
       " 413: 'fucking',\n",
       " 414: 'smelled',\n",
       " 415: 'intermittently',\n",
       " 416: 'gay',\n",
       " 417: 'jimmy',\n",
       " 418: 'a',\n",
       " 419: 'dosent',\n",
       " 420: 'crew',\n",
       " 421: 'lane',\n",
       " 422: 'parker',\n",
       " 423: 'piece',\n",
       " 424: 'positive',\n",
       " 425: 'wouldbe',\n",
       " 426: 'mains',\n",
       " 427: 'sergeant',\n",
       " 428: 'valentine',\n",
       " 429: 'describing',\n",
       " 430: 'towards',\n",
       " 431: 'problems\\x97the',\n",
       " 432: 'attached',\n",
       " 433: 'amusing',\n",
       " 434: 'sloppy',\n",
       " 435: 'cocktails',\n",
       " 436: 'cancan',\n",
       " 437: 'impeccable',\n",
       " 438: 'empowerment',\n",
       " 439: 'originally',\n",
       " 440: 'charisma',\n",
       " 441: 'car',\n",
       " 442: 'garage',\n",
       " 443: 'following',\n",
       " 444: 'translating',\n",
       " 445: 'brat',\n",
       " 446: 'batteries',\n",
       " 447: 'stratus',\n",
       " 448: 'omelets',\n",
       " 449: 'clicks',\n",
       " 450: 'defensemen',\n",
       " 451: 'bill',\n",
       " 452: 'remotely',\n",
       " 453: 'receptionsound',\n",
       " 454: 'real',\n",
       " 455: 'initially',\n",
       " 456: 'africa',\n",
       " 457: 'end',\n",
       " 458: '40',\n",
       " 459: 'hearts',\n",
       " 460: 'clips',\n",
       " 461: 'coherent',\n",
       " 462: 'nut',\n",
       " 463: 'struggle',\n",
       " 464: 'behold',\n",
       " 465: 'sitting',\n",
       " 466: 'dieing',\n",
       " 467: 'menacing',\n",
       " 468: 'cartel',\n",
       " 469: 'mango',\n",
       " 470: 'angle',\n",
       " 471: 'that',\n",
       " 472: 'stayed',\n",
       " 473: 'disgusted',\n",
       " 474: 'broke',\n",
       " 475: 'onethis',\n",
       " 476: 'followed',\n",
       " 477: 'entrance',\n",
       " 478: 'logitech',\n",
       " 479: 'joke',\n",
       " 480: 'james',\n",
       " 481: 'dont',\n",
       " 482: 'member',\n",
       " 483: 'rests',\n",
       " 484: 'sequels',\n",
       " 485: 'brilliantly',\n",
       " 486: 'forced',\n",
       " 487: 'spot',\n",
       " 488: 'struck',\n",
       " 489: 'delicate',\n",
       " 490: 'nay',\n",
       " 491: 'thinly',\n",
       " 492: 'shiny',\n",
       " 493: 'nonsequel',\n",
       " 494: 'unpredictability',\n",
       " 495: 'eve',\n",
       " 496: 'funnyall',\n",
       " 497: '6',\n",
       " 498: 'en',\n",
       " 499: 'challenges',\n",
       " 500: 'chefs',\n",
       " 501: 'getting',\n",
       " 502: 'tater',\n",
       " 503: 'within',\n",
       " 504: 'native',\n",
       " 505: 'howe',\n",
       " 506: 'ringer',\n",
       " 507: 'miniseries',\n",
       " 508: 'indoors',\n",
       " 509: 'gibberish',\n",
       " 510: 'realworld',\n",
       " 511: 'farce',\n",
       " 512: 'blow',\n",
       " 513: 'alike',\n",
       " 514: 'moz',\n",
       " 515: 'baaaaaad',\n",
       " 516: 'uncomfortable',\n",
       " 517: 'theme',\n",
       " 518: 'uninteresting',\n",
       " 519: 'help',\n",
       " 520: 'production',\n",
       " 521: 'gx2',\n",
       " 522: 'veteran',\n",
       " 523: 'man',\n",
       " 524: 'groove',\n",
       " 525: 'anatomist',\n",
       " 526: 'tied',\n",
       " 527: 'questioning',\n",
       " 528: 'slightly',\n",
       " 529: 'timely',\n",
       " 530: 'ugly',\n",
       " 531: 'medium',\n",
       " 532: 'eat',\n",
       " 533: 'dysfunctionhe',\n",
       " 534: 'stays',\n",
       " 535: 'making',\n",
       " 536: 'wasting',\n",
       " 537: 'content',\n",
       " 538: 'latterday',\n",
       " 539: 'familiar',\n",
       " 540: 'bold',\n",
       " 541: 'solid',\n",
       " 542: 'blake',\n",
       " 543: 'mother',\n",
       " 544: 'childhood',\n",
       " 545: 'thrillerhorror',\n",
       " 546: 'donut',\n",
       " 547: 'tender',\n",
       " 548: 'accessable',\n",
       " 549: 'i',\n",
       " 550: 'computer',\n",
       " 551: 'reoccurebottom',\n",
       " 552: 'hackneyed',\n",
       " 553: 'idiotic',\n",
       " 554: 'baba',\n",
       " 555: 'cartoons',\n",
       " 556: 'cailles',\n",
       " 557: 'gerardo',\n",
       " 558: 'an',\n",
       " 559: 'easy',\n",
       " 560: 'well',\n",
       " 561: 'couple',\n",
       " 562: 'patriotism',\n",
       " 563: 'ethic',\n",
       " 564: 'charismafree',\n",
       " 565: 'lines',\n",
       " 566: 'restaraunt',\n",
       " 567: 'spock',\n",
       " 568: 'grandmother',\n",
       " 569: 'body',\n",
       " 570: 'jamaican',\n",
       " 571: 'unbelievable',\n",
       " 572: '18th',\n",
       " 573: 'attempting',\n",
       " 574: 'sour',\n",
       " 575: 'cibo',\n",
       " 576: '325',\n",
       " 577: 'levels',\n",
       " 578: 'choked',\n",
       " 579: 'must',\n",
       " 580: 'pause',\n",
       " 581: 'least',\n",
       " 582: 'guests',\n",
       " 583: 'enough',\n",
       " 584: 'despite',\n",
       " 585: 'stream',\n",
       " 586: 'spoiler',\n",
       " 587: 'wants',\n",
       " 588: 'drinking',\n",
       " 589: 'pay',\n",
       " 590: 'thatsucked',\n",
       " 591: 'knightley',\n",
       " 592: 'slider',\n",
       " 593: 'cramming',\n",
       " 594: 'each',\n",
       " 595: 'skype',\n",
       " 596: 'laptop',\n",
       " 597: 'serves',\n",
       " 598: '3',\n",
       " 599: 'leni',\n",
       " 600: 'described',\n",
       " 601: 'predictable',\n",
       " 602: 'readers',\n",
       " 603: 'not',\n",
       " 604: 'portable',\n",
       " 605: 'couples',\n",
       " 606: 'hunan',\n",
       " 607: 'inch',\n",
       " 608: 'holder',\n",
       " 609: 'tepid',\n",
       " 610: 'stinks',\n",
       " 611: 'cg',\n",
       " 612: 'figured',\n",
       " 613: 'aversion',\n",
       " 614: 'wind',\n",
       " 615: 'concentrate',\n",
       " 616: 'problem',\n",
       " 617: 'whos',\n",
       " 618: 'concrete',\n",
       " 619: 'average',\n",
       " 620: 'bechard',\n",
       " 621: 'packed',\n",
       " 622: 'transformed',\n",
       " 623: 'soyo',\n",
       " 624: 'tasteless',\n",
       " 625: 'act',\n",
       " 626: 'eye',\n",
       " 627: 'madhouse',\n",
       " 628: 'ceases',\n",
       " 629: 'couldnt',\n",
       " 630: 'backlight',\n",
       " 631: 'tiramisu',\n",
       " 632: 'actions',\n",
       " 633: 'fire',\n",
       " 634: 'lukewarm',\n",
       " 635: 'deadpan',\n",
       " 636: 'directtovideo',\n",
       " 637: 'remember',\n",
       " 638: 'weaker',\n",
       " 639: 'lets',\n",
       " 640: 'betty',\n",
       " 641: 'up',\n",
       " 642: 'actually',\n",
       " 643: 'working',\n",
       " 644: 'idyllic',\n",
       " 645: 'though',\n",
       " 646: 'buffets',\n",
       " 647: 'punishment',\n",
       " 648: 'service',\n",
       " 649: 'secondly',\n",
       " 650: 'ed',\n",
       " 651: 'belly',\n",
       " 652: 'kevin',\n",
       " 653: 'chickenwith',\n",
       " 654: 'highlight',\n",
       " 655: 'exterior',\n",
       " 656: 'management',\n",
       " 657: 'disgust',\n",
       " 658: 'handy',\n",
       " 659: 'be',\n",
       " 660: 'bartender',\n",
       " 661: 'relation',\n",
       " 662: 'operates',\n",
       " 663: 'francisco',\n",
       " 664: 'cgi',\n",
       " 665: 'putting',\n",
       " 666: 'belmondo',\n",
       " 667: 'razor',\n",
       " 668: 'standard',\n",
       " 669: 'entrees',\n",
       " 670: 'furthermore',\n",
       " 671: 'texas',\n",
       " 672: 'sculpture',\n",
       " 673: 'avocado',\n",
       " 674: 'bloodiest',\n",
       " 675: 'postinos',\n",
       " 676: 'accomodate',\n",
       " 677: 'vomited',\n",
       " 678: 'friendship',\n",
       " 679: 'nan',\n",
       " 680: 'middle',\n",
       " 681: 'suffered',\n",
       " 682: 'casing',\n",
       " 683: 'lousy',\n",
       " 684: 'schizophrenic',\n",
       " 685: 'complex',\n",
       " 686: 'actresses',\n",
       " 687: 'transceiver',\n",
       " 688: 'hinge',\n",
       " 689: 'hatred',\n",
       " 690: 'deliver',\n",
       " 691: 'handles',\n",
       " 692: 'market',\n",
       " 693: 'revisiting',\n",
       " 694: 'lord',\n",
       " 695: 'breeders',\n",
       " 696: 'asking',\n",
       " 697: 'external',\n",
       " 698: 'spice',\n",
       " 699: 'mouth',\n",
       " 700: 'itmy',\n",
       " 701: 'redeemed',\n",
       " 702: 'unconditional',\n",
       " 703: 'soggy',\n",
       " 704: 'defective',\n",
       " 705: 'excrutiatingly',\n",
       " 706: 'whoa',\n",
       " 707: 'suddenly',\n",
       " 708: 'wih',\n",
       " 709: 'cinematographyif',\n",
       " 710: 'plantains',\n",
       " 711: 'ad',\n",
       " 712: 'screams',\n",
       " 713: 'bussell',\n",
       " 714: 'bored',\n",
       " 715: 'wiping',\n",
       " 716: 'celebration',\n",
       " 717: 'jawbone',\n",
       " 718: 'late',\n",
       " 719: 'informative',\n",
       " 720: 'less',\n",
       " 721: 'dickens',\n",
       " 722: 'grew',\n",
       " 723: 'different',\n",
       " 724: 'double',\n",
       " 725: 'spacey',\n",
       " 726: 'haggis',\n",
       " 727: 'andor',\n",
       " 728: 'soooooo',\n",
       " 729: 'anytime',\n",
       " 730: 'ford',\n",
       " 731: 'achievement',\n",
       " 732: 'sharp',\n",
       " 733: 'cheap',\n",
       " 734: 'grocery',\n",
       " 735: 'recall',\n",
       " 736: 'thrown',\n",
       " 737: 'unoriginal',\n",
       " 738: 'later',\n",
       " 739: 'misplace',\n",
       " 740: 'dustin',\n",
       " 741: 'subtitles',\n",
       " 742: 'afternoon',\n",
       " 743: 'igo',\n",
       " 744: 'stari',\n",
       " 745: 'selection',\n",
       " 746: 'take',\n",
       " 747: 'mebunch',\n",
       " 748: 'pitch',\n",
       " 749: 'thus',\n",
       " 750: 'motorolas',\n",
       " 751: 'overhaul',\n",
       " 752: 'cancelling',\n",
       " 753: 'individual',\n",
       " 754: 'presents',\n",
       " 755: 'ursula',\n",
       " 756: 'paying',\n",
       " 757: 'connections',\n",
       " 758: 'moist',\n",
       " 759: 'madison',\n",
       " 760: 'patient',\n",
       " 761: 'repeating',\n",
       " 762: 'stop',\n",
       " 763: 'trumpeter',\n",
       " 764: 'bird',\n",
       " 765: 'bubbling',\n",
       " 766: 'screened',\n",
       " 767: 'similarly',\n",
       " 768: 'freeman',\n",
       " 769: 'toactivate',\n",
       " 770: 'lg',\n",
       " 771: 'pricing',\n",
       " 772: '30s',\n",
       " 773: '1949',\n",
       " 774: 'dennys',\n",
       " 775: 'drastically',\n",
       " 776: 'swamp',\n",
       " 777: 'hate',\n",
       " 778: 'floppy',\n",
       " 779: 'estate',\n",
       " 780: 'yellow',\n",
       " 781: 'crystals',\n",
       " 782: 'failed',\n",
       " 783: 'creates',\n",
       " 784: 'keys',\n",
       " 785: 'set',\n",
       " 786: 'websites',\n",
       " 787: 'windows',\n",
       " 788: 'replaceeasy',\n",
       " 789: 'itself',\n",
       " 790: 'monkeys',\n",
       " 791: 'gon',\n",
       " 792: 'startac',\n",
       " 793: 'establishment',\n",
       " 794: 'common',\n",
       " 795: 'pale',\n",
       " 796: 'decisions',\n",
       " 797: 'shipment',\n",
       " 798: 'junkyard',\n",
       " 799: 'ok',\n",
       " 800: 'watered',\n",
       " 801: 'decent',\n",
       " 802: 'modest',\n",
       " 803: 'turkey',\n",
       " 804: 'repair',\n",
       " 805: 'inspiring',\n",
       " 806: 'rocketed',\n",
       " 807: 'excuse',\n",
       " 808: 'dish',\n",
       " 809: 'proven',\n",
       " 810: 'brought',\n",
       " 811: 'reviewer',\n",
       " 812: 'effect',\n",
       " 813: 'estevezs',\n",
       " 814: 'color',\n",
       " 815: 'efforts',\n",
       " 816: 'dylan',\n",
       " 817: 'tricky',\n",
       " 818: 'negulesco',\n",
       " 819: 'choose',\n",
       " 820: 'house',\n",
       " 821: 'user',\n",
       " 822: 'wonderfully',\n",
       " 823: 'steve',\n",
       " 824: 'refurb',\n",
       " 825: 'factor',\n",
       " 826: 'conversations',\n",
       " 827: 'mst3k',\n",
       " 828: 'cool',\n",
       " 829: 'lasted',\n",
       " 830: 'reaching',\n",
       " 831: 'embarrassed',\n",
       " 832: 'himself',\n",
       " 833: 'voiceovers',\n",
       " 834: 'hasnt',\n",
       " 835: 'austens',\n",
       " 836: 'badwellits',\n",
       " 837: 'dispenser',\n",
       " 838: 'flaw',\n",
       " 839: 'hundred',\n",
       " 840: 'bluegreenscreen',\n",
       " 841: 'floor',\n",
       " 842: 'believe',\n",
       " 843: 'notch',\n",
       " 844: 'surprise',\n",
       " 845: 'astronauts',\n",
       " 846: 'reporter',\n",
       " 847: 'dropped',\n",
       " 848: 'hugo',\n",
       " 849: 'bread',\n",
       " 850: 'intoning',\n",
       " 851: 'warn',\n",
       " 852: 'comment',\n",
       " 853: 'flakes',\n",
       " 854: 'mirage',\n",
       " 855: 'cheaper',\n",
       " 856: 'creature',\n",
       " 857: 'zero',\n",
       " 858: 'witticisms',\n",
       " 859: 'moral',\n",
       " 860: 'courtroom',\n",
       " 861: 'too',\n",
       " 862: 'screenthis',\n",
       " 863: 'down',\n",
       " 864: 'allstar',\n",
       " 865: 'alot',\n",
       " 866: 'truffle',\n",
       " 867: 'mouthful',\n",
       " 868: 'armband',\n",
       " 869: 'fair',\n",
       " 870: 'pine',\n",
       " 871: 'charcoal',\n",
       " 872: 'tries',\n",
       " 873: 'qualified',\n",
       " 874: 'bose',\n",
       " 875: 'google',\n",
       " 876: 'theatre',\n",
       " 877: 'manual',\n",
       " 878: 'dos',\n",
       " 879: 'trouble',\n",
       " 880: 'painted',\n",
       " 881: 'roosevelts',\n",
       " 882: 'commented',\n",
       " 883: 'homemade',\n",
       " 884: 'slimy',\n",
       " 885: 'melt',\n",
       " 886: 'hour',\n",
       " 887: 'greedy',\n",
       " 888: 'crostini',\n",
       " 889: 'absolutel',\n",
       " 890: 'eyepleasing',\n",
       " 891: 'miserably',\n",
       " 892: 'retreat',\n",
       " 893: 'shots',\n",
       " 894: 'considered',\n",
       " 895: 'captures',\n",
       " 896: 'salesman',\n",
       " 897: 'paid',\n",
       " 898: 'lifemy',\n",
       " 899: 'wifetobe',\n",
       " 900: 'hopeless',\n",
       " 901: 'childrens',\n",
       " 902: 'unfortunate',\n",
       " 903: 'gadgets',\n",
       " 904: 'beforei',\n",
       " 905: 'pairing',\n",
       " 906: 'either',\n",
       " 907: 'absolutley',\n",
       " 908: 'above',\n",
       " 909: 'portraying',\n",
       " 910: 'helms',\n",
       " 911: 'damn',\n",
       " 912: 'ravoli',\n",
       " 913: 'worldweariness',\n",
       " 914: 'jerky',\n",
       " 915: 'maybe',\n",
       " 916: 'confuses',\n",
       " 917: 'spots',\n",
       " 918: 'flavorless',\n",
       " 919: 'sources',\n",
       " 920: 'displeased',\n",
       " 921: 'friends',\n",
       " 922: 'reviewing',\n",
       " 923: 'division',\n",
       " 924: 'disgusting',\n",
       " 925: 'suited',\n",
       " 926: 'smoke',\n",
       " 927: 'missing',\n",
       " 928: 'tape',\n",
       " 929: 'wifi',\n",
       " 930: 'jokes',\n",
       " 931: 'share',\n",
       " 932: 'notice',\n",
       " 933: 'rivalry',\n",
       " 934: 'dominated',\n",
       " 935: 'juano',\n",
       " 936: 'disbelief',\n",
       " 937: 'helping',\n",
       " 938: 'itbuy',\n",
       " 939: 'suck',\n",
       " 940: 'mindbendingly',\n",
       " 941: 'decision',\n",
       " 942: 'yelps',\n",
       " 943: 'employees',\n",
       " 944: 'env',\n",
       " 945: 'immediately',\n",
       " 946: 'bisque',\n",
       " 947: 'magnificent',\n",
       " 948: 'managed',\n",
       " 949: 'yelpers',\n",
       " 950: 'breaking',\n",
       " 951: 'girls',\n",
       " 952: 'lestat',\n",
       " 953: 'folks',\n",
       " 954: 'amateurish',\n",
       " 955: 'dime',\n",
       " 956: 'lemon',\n",
       " 957: 'view',\n",
       " 958: 'muststop',\n",
       " 959: 'spent',\n",
       " 960: 'sheer',\n",
       " 961: 'egotism',\n",
       " 962: 'june',\n",
       " 963: 'out',\n",
       " 964: 'murdering',\n",
       " 965: 'both',\n",
       " 966: 'flimsy',\n",
       " 967: 'gyros',\n",
       " 968: 'their',\n",
       " 969: 'has',\n",
       " 970: 'pepper',\n",
       " 971: 'vx',\n",
       " 972: 'publicly',\n",
       " 973: 'martini',\n",
       " 974: 'scale',\n",
       " 975: 'fooled',\n",
       " 976: 'fries',\n",
       " 977: 'shepards',\n",
       " 978: 'educational',\n",
       " 979: 'malebonding',\n",
       " 980: '20th',\n",
       " 981: 'contact',\n",
       " 982: 'after',\n",
       " 983: 'increase',\n",
       " 984: 'goat',\n",
       " 985: 'acted',\n",
       " 986: 'refrained',\n",
       " 987: 'salad',\n",
       " 988: 'wings',\n",
       " 989: 'port',\n",
       " 990: 'factory',\n",
       " 991: 'beensteppedinandtrackedeverywhere',\n",
       " 992: 'knock',\n",
       " 993: 'lock',\n",
       " 994: 'often',\n",
       " 995: 'southern',\n",
       " 996: 'supposedly',\n",
       " 997: 'killing',\n",
       " 998: 'gradually',\n",
       " 999: 'scripted',\n",
       " 1000: 'weight',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting works to numbers (embeddings for corpus)\n",
    "def create_dictionaries(words):\n",
    "    word_to_int_dict = {w:i+1 for i, w in enumerate(words)}\n",
    "    int_to_word_dict = {i:w for w, i in word_to_int_dict.items()}\n",
    "    return word_to_int_dict, int_to_word_dict\n",
    "\n",
    "word_to_int_dict, int_to_word_dict = create_dictionaries(vocab)\n",
    "\n",
    "int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e5c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_int_dict.json', 'w') as fp:\n",
    "    json.dump(word_to_int_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e836a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "11.783666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n- Note that NNs will take inputs of a fixed length\\n- but our reviews are all of diff lengths\\n- therefore, we have to add padding (adding empty tokens)\\n\\nHOWEVER\\n- longer sentence will made LSTM layer deeper, in turn makes backprop training longer\\n- large % of our input would be sparse and empty tokens\\n- so make input size 50 (between 20-70)\\n\\nLogic\\n- for reviews longer than 50, drop the rest of the token\\n- for reviews shorter than 50, add empty padded tokens\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max([len(x) for x in reviews]))\n",
    "print(np.mean([len(x) for x in reviews]))\n",
    "\n",
    "'''\n",
    "- Note that NNs will take inputs of a fixed length\n",
    "- but our reviews are all of diff lengths\n",
    "- therefore, we have to add padding (adding empty tokens)\n",
    "\n",
    "HOWEVER\n",
    "- longer sentence will made LSTM layer deeper, in turn makes backprop training longer\n",
    "- large % of our input would be sparse and empty tokens\n",
    "- so make input size 50 (between 20-70)\n",
    "\n",
    "Logic\n",
    "- for reviews longer than 50, drop the rest of the token\n",
    "- for reviews shorter than 50, add empty padded tokens\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d7482cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', 'the', 'warmth', 'it', 'generates', 'is', 'in',\n",
       "       'contrast', 'to', 'its', 'austere', 'backdrop'], dtype='<U33')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_text(tokenized_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append(['']*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "padded_sentences = pad_text(reviews, seq_length = 50)\n",
    "\n",
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcfc9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "need to assign what empty token means in our model, so use 0\n",
    "'''\n",
    "int_to_word_dict[0] = ''\n",
    "word_to_int_dict[''] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ec4254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0, 2271, 1732, 3564, 2325,  335,\n",
       "       4336, 4034, 4945, 4127, 4628,  225])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "encoding our padded sentence into numeric vectors for feeding into model,\n",
    "similar to bag of words\n",
    "'''\n",
    "encoded_sentences = np.array([[word_to_int_dict[word] for word in review] for review in padded_sentences])\n",
    "\n",
    "encoded_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da4dbf",
   "metadata": {},
   "source": [
    "\n",
    "### Model Architecture\n",
    "\n",
    "   - Input layer\n",
    "   - Embedding layer:\n",
    "        - model learns the vector representation of the words that its being trained on\n",
    "        - use precomputed embeddings (like GLoVe)\n",
    "        - tho using our own embedding layer\n",
    "        - our input sequences are fed through the layers and come out as a seq. on vectors\n",
    "        - this vector seq in then fed into out LSTM layer\n",
    "   - LSTM layer:\n",
    "        - the LSTM layers learns sequentially from seq. of embeddings and outputs a single vector representation of the final hidden state of LSTM\n",
    "\n",
    "   - Final HS layer:\n",
    "       - the HS vector gets fed here\n",
    "       - will follow standard NN architecture from here on \n",
    "   - FC layer:\n",
    "       - complexities of model architecture based on data\n",
    "   - Classification/Output layer:\n",
    "       - since classification, has one node with 0 and 1 prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60041553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    #init as size of vocab, # of LSTM layers, size of models HS\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab  \n",
    "        self.n_layers = n_layers \n",
    "        self.n_hidden = n_hidden \n",
    "        \n",
    "        #embedding layers have the length of # of words in vocab and size of embed vect\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                          \n",
    "                          \n",
    "        embedded_words = self.embedding(input_words)\n",
    "        lstm_out, h = self.lstm(embedded_words) \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) #using view to reshape tensor\n",
    "        fc_out = self.fc(lstm_out)                  \n",
    "        sigmoid_out = self.sigmoid(fc_out)              \n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  \n",
    "    \n",
    "        sigmoid_last = sigmoid_out[:, -1].squeeze()  # squeezing the last output to remove extra dimension\n",
    "    \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    #init hidden layers w dim. of batch size\n",
    "    #allows model to train/pred on many sentences at once than training sequentially\n",
    "    def init_hidden (self, batch_size):\n",
    "        \n",
    "        device = \"cuda\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc4e23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising our model\n",
    "\n",
    "n_vocab = len(word_to_int_dict)\n",
    "n_embed = 50\n",
    "n_hidden = 100\n",
    "n_output = 1\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da90314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "# Train/Valid/Test set split 80/10/10\n",
    "\n",
    "labels = np.array([int(x) for x in data['Sentiment'].values])\n",
    "\n",
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio)/2\n",
    "\n",
    "#ratio slicing\n",
    "total = len(encoded_sentences)\n",
    "train_cutoff = int(total * train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "\n",
    "train_x, train_y = torch.Tensor(encoded_sentences[:train_cutoff]).long(), torch.Tensor(labels[:train_cutoff]).long()\n",
    "valid_x, valid_y = torch.Tensor(encoded_sentences[train_cutoff : valid_cutoff]).long(), torch.Tensor(labels[train_cutoff : valid_cutoff]).long()\n",
    "test_x, test_y = torch.Tensor(encoded_sentences[valid_cutoff:]).long(), torch.Tensor(labels[valid_cutoff:])\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "\n",
    "#use the split dataset to create PyTorch DataLoader object\n",
    "# allows us to batch process\n",
    "#randomly shuffled (removes bias from training order)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "663ea8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 2400\n",
    "step = 0\n",
    "n_epochs = 3\n",
    "clip = 5  #grad clipping\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy, as we are dealing with pred. single bin. class\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aa5abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Step: 4800 Training Loss: 2.1769 Validation Loss: 0.6860\n",
      "Epoch: 2/3 Step: 7200 Training Loss: 0.0487 Validation Loss: 0.8035\n",
      "Epoch: 3/3 Step: 9600 Training Loss: 0.0001 Validation Loss: 1.0089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1  \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "\n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                       \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output, v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52ace3",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- extremely overfitting on the training data\n",
    "- result of small data set \n",
    "- and in embedd. layer, words occur only ince in training set and never in the valid set\n",
    "- ideally need a much bigger set for better generalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "678f43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "761431e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.load_state_dict(torch.load('model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c654fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'net.eval()\\ntest_losses = []\\nnum_correct = 0\\n\\nfor inputs, labels in test_loader:\\n\\n    test_output, test_h = net(inputs)\\n    loss = criterion(test_output, labels.float())\\n    test_losses.append(loss.item())\\n    \\n    preds = torch.round(test_output.squeeze())\\n    correct_tensor = preds.eq(labels.float().view_as(preds))\\n    correct = np.squeeze(correct_tensor.numpy())\\n    num_correct += np.sum(correct)\\n    \\nprint(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\\nprint(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))    '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    test_output, test_h = net(inputs)\n",
    "    loss = criterion(test_output, labels.float())\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49551033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    review = review.translate(str.maketrans('', '', punctuation)).lower().rstrip()\n",
    "    tokenized = word_tokenize(review)\n",
    "    if len(tokenized) >= 50:\n",
    "        review = tokenized[:50]\n",
    "    else:\n",
    "        review= ['0']*(50-len(tokenized)) + tokenized\n",
    "    \n",
    "    final = []\n",
    "    \n",
    "    for token in review:\n",
    "        try:\n",
    "            final.append(word_to_int_dict[token])\n",
    "            \n",
    "        except:\n",
    "            final.append(word_to_int_dict[''])\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def predict(review):\n",
    "    net.eval()\n",
    "    words = np.array([preprocess_review(review)])\n",
    "    padded_words = torch.from_numpy(words)\n",
    "    pred_loader = DataLoader(padded_words, batch_size = 1, shuffle = True)\n",
    "    for x in pred_loader:\n",
    "        output = net(x)[0].item()\n",
    "    \n",
    "    msg = \"This is a positive review.\" if output >= 0.5 else \"This is a negative review.\"\n",
    "    print(msg)\n",
    "    print('Prediction = ' + str(output))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c53425",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''predict(\"The film was good\")\n",
    "predict(\"It was not good\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080468c",
   "metadata": {},
   "source": [
    "## Deploying app on Heroku"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
