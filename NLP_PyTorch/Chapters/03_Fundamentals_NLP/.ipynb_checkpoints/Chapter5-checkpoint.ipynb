{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cd61b1",
   "metadata": {},
   "source": [
    "# RNNs & Sentiment Analysis\n",
    "\n",
    "- RecurrentNN variation of the FF-NNs\n",
    "- used when task can be represented as a sequence\n",
    "- sentence is a seq of words\n",
    "- RNN takes whole sequence of vectors as input (CNN takes single vector)\n",
    "- if each word in document is a vector embedd, then whole document can be represented as order 3 tensor\n",
    "- LSTM (a more sophisticated RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd7c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e9f9d",
   "metadata": {},
   "source": [
    "### Theory of Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- RNNs are structured with recurrent layers, similar to standard feedforward neural networks (NNs).\n",
    "- They incorporate a hidden recurrent state that gets updated at each step during sequence processing.\n",
    "- At the start of processing any sequence, the model is initialized with a one-dimensional vector representing the hidden state.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "- Each word in the sequence is fed into the model, leading to an update in the hidden state (HS).\n",
    "- This process continues until all words have been processed, generating a final hidden state vector.\n",
    "- This final HS vector is then fed into a fully connected layer to yield the final class prediction.\n",
    "- Activation functions, such as tanh, can be employed to constrain the hidden state values between -1 and 1.\n",
    "- During learning, weights are updated at each step, and the loss is computed during backpropagation.\n",
    "- For tasks such as sequence-to-sequence translation in NLP, hidden state values from each layer can be utilized instead of only the final HS.\n",
    "\n",
    "### RNN for Sentiment Analysis\n",
    "\n",
    "- Sentiment analysis becomes a binary classification task (i.e., positive or negative) when using RNNs.\n",
    "\n",
    "### Potential Challenges and Solutions\n",
    "\n",
    "- One of the challenges with RNNs is the occurrence of exploding or vanishing gradients due to the recursive layers, which can cause instability in the network.\n",
    "- Solutions include:\n",
    "  - Implementing gradient clipping to limit the gradients from becoming excessively large. This involves introducing a hyperparameter C to establish an upper limit.\n",
    "  - Reducing the input sequence length. Since a shorter sequence means fewer iterations, the maximum sequence length can be chosen as a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775ee5f",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Flaws of RNN\n",
    "- hard to retain information long term (cant capture long-term sentence dependency)\n",
    "- poor at capturing context of word within sentence (lacing context dependency, due to how its trained)\n",
    "- unable to predict things that came early on in the sentence due to it being trained in one directional\n",
    "\n",
    "LSTM can combat these issues\n",
    "- a LSTM is a more sophisticated RNN and contains 2 extra propertises (an update gate and a forget gate)\n",
    "- these 2 additions, make it easier to learn long term dependencies\n",
    "- in context of Sentiment Analysis, LSTM will remember important information and leave our irrelevant ones, preventing the irrelevent features to dilute important information, maintaining long term dependencies\n",
    "\n",
    "- LSTM has a similar strcuture to RNN with the recursive hidden state but the LSTM cell is more complex\n",
    "- it has a series of gates that allow for the additional calculations. lets break it down\n",
    "\n",
    "#### forget gate\n",
    "- learns which elements of the sequence to forget\n",
    "- the previous hidden state (h. t-1) and the latest input step (x1) and concat. together and pass through a matrix of learned weights on the forget gate\n",
    "a sigmoid function the bounds the value between 0 and 1\n",
    "- This resulting matrix, ft, is multiplied pointwise by the cell state from the previous step, ct-1. \n",
    "- This effectively applies a mask to the previous cell state so that only the relevant information from the previous cell state is brought forward.\n",
    "\n",
    "#### input gate\n",
    "- takes in concat. input and pass it into a sigmoid function to bound between 0 and 1\n",
    "\n",
    "#### output gate\n",
    "- calculates final output out of the LSTM cell\n",
    "- learned para on output gate control which elemets of the previous HS and current output and combined to then carry forward to the next stage\n",
    "\n",
    "- in one forward pass, we iterate through the model, init hidden state, cell stae and update at each stem\n",
    "- backprop used to calculate gradients relative to loss, to know which direction to update our para\n",
    "- LSTM has more computations than RNN, so more complex computation graph \n",
    "- and backprop calculation for gradient will also take longer\n",
    "- but despite longer time, LSTM offers signifant improve in performance compared to RNN\n",
    "- this is because the 3 gates give the model ability to determine which elements of the input should be used to udpate the hidden state etc\n",
    "- which means model is better at forming long term dependencies and retain information from prev steps\n",
    "\n",
    "### Bidirectional LSTMs\n",
    "- modifed LSTM that considers both the words before and adter it at each step within sequence\n",
    "- LSTMMs process seq in regular order and reserve order simultaniosly, maintaining 2 hidden states\n",
    "- this allows for the context of any given word within seq can be better captured\n",
    "- Bidirectional LSTM offer improved performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54370fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a3ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36220e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
